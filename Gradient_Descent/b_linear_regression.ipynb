{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee9ba2f",
   "metadata": {},
   "source": [
    "# Linear Regression with Gradient Descent \n",
    "In this notebook we extend our previous implementation of **general function via gradient descent**  to build a **linear regression gradient descent** from scratch, using only NumPy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15279e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc235618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a linear model\n",
    "def linear_model(x, m, b):\n",
    "    return m*x + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b3f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (mean squared error)\n",
    "def mse_loss(m, b):\n",
    "    return np.mean((Y - linear_model(X, m, b))**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a6806b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss is 1766.5031899691112\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enhanced dataset: y = 2.5x + 7 + noise\n",
    "X = np.linspace(0, 20, 50)\n",
    "Y = 2.5 * X + 7 + np.random.normal(0, 4, size=X.shape)\n",
    "\n",
    "# Initialize parameters\n",
    "# Random initialization of m and b\n",
    "\n",
    "m = np.random.uniform(-1, 1)\n",
    "b = np.random.uniform(-1, 1)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "iterations = 200\n",
    "y_pred = linear_model(X, m, b)\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(X, y_pred, color='red', label=f'Initial line: y={m:.2f}x+{b:.2f}')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total loss is {mse_loss(m,b)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122633f",
   "metadata": {},
   "source": [
    "- we can see here that our total loss is so high, our goal is to minimize it with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914437a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting with m: -0.72, b: 0.60, now our total loss is 1766.50\n",
      "we're in iteration #1...\n",
      "updating m: -0.62, updating b: 0.61, now our total loss is 1673.36\n",
      "we're in iteration #2...\n",
      "updating m: -0.53, updating b: 0.62, now our total loss is 1585.20\n",
      "we're in iteration #3...\n",
      "updating m: -0.44, updating b: 0.63, now our total loss is 1501.75\n",
      "we're in iteration #4...\n",
      "updating m: -0.35, updating b: 0.63, now our total loss is 1422.76\n",
      "we're in iteration #5...\n",
      "updating m: -0.26, updating b: 0.64, now our total loss is 1348.00\n",
      "we're in iteration #6...\n",
      "updating m: -0.17, updating b: 0.65, now our total loss is 1277.22\n",
      "we're in iteration #7...\n",
      "updating m: -0.09, updating b: 0.65, now our total loss is 1210.23\n",
      "we're in iteration #8...\n",
      "updating m: -0.01, updating b: 0.66, now our total loss is 1146.82\n",
      "we're in iteration #9...\n",
      "updating m: 0.06, updating b: 0.67, now our total loss is 1086.80\n",
      "we're in iteration #10...\n",
      "updating m: 0.14, updating b: 0.67, now our total loss is 1029.98\n",
      "we're in iteration #11...\n",
      "updating m: 0.21, updating b: 0.68, now our total loss is 976.21\n",
      "we're in iteration #12...\n",
      "updating m: 0.29, updating b: 0.68, now our total loss is 925.30\n",
      "we're in iteration #13...\n",
      "updating m: 0.36, updating b: 0.69, now our total loss is 877.12\n",
      "we're in iteration #14...\n",
      "updating m: 0.42, updating b: 0.69, now our total loss is 831.51\n",
      "we're in iteration #15...\n",
      "updating m: 0.49, updating b: 0.70, now our total loss is 788.34\n",
      "we're in iteration #16...\n",
      "updating m: 0.55, updating b: 0.70, now our total loss is 747.47\n",
      "we're in iteration #17...\n",
      "updating m: 0.62, updating b: 0.71, now our total loss is 708.79\n",
      "we're in iteration #18...\n",
      "updating m: 0.68, updating b: 0.71, now our total loss is 672.18\n",
      "we're in iteration #19...\n",
      "updating m: 0.74, updating b: 0.72, now our total loss is 637.52\n",
      "we're in iteration #20...\n",
      "updating m: 0.79, updating b: 0.72, now our total loss is 604.72\n",
      "we're in iteration #21...\n",
      "updating m: 0.85, updating b: 0.73, now our total loss is 573.66\n",
      "we're in iteration #22...\n",
      "updating m: 0.90, updating b: 0.73, now our total loss is 544.27\n",
      "we're in iteration #23...\n",
      "updating m: 0.96, updating b: 0.74, now our total loss is 516.45\n",
      "we're in iteration #24...\n",
      "updating m: 1.01, updating b: 0.74, now our total loss is 490.11\n",
      "we're in iteration #25...\n",
      "updating m: 1.06, updating b: 0.74, now our total loss is 465.19\n",
      "we're in iteration #26...\n",
      "updating m: 1.11, updating b: 0.75, now our total loss is 441.59\n",
      "we're in iteration #27...\n",
      "updating m: 1.15, updating b: 0.75, now our total loss is 419.26\n",
      "we're in iteration #28...\n",
      "updating m: 1.20, updating b: 0.76, now our total loss is 398.12\n",
      "we're in iteration #29...\n",
      "updating m: 1.25, updating b: 0.76, now our total loss is 378.10\n",
      "we're in iteration #30...\n",
      "updating m: 1.29, updating b: 0.76, now our total loss is 359.16\n",
      "we're in iteration #31...\n",
      "updating m: 1.33, updating b: 0.77, now our total loss is 341.23\n",
      "we're in iteration #32...\n",
      "updating m: 1.37, updating b: 0.77, now our total loss is 324.26\n",
      "we're in iteration #33...\n",
      "updating m: 1.41, updating b: 0.77, now our total loss is 308.20\n",
      "we're in iteration #34...\n",
      "updating m: 1.45, updating b: 0.78, now our total loss is 292.99\n",
      "we're in iteration #35...\n",
      "updating m: 1.49, updating b: 0.78, now our total loss is 278.60\n",
      "we're in iteration #36...\n",
      "updating m: 1.53, updating b: 0.78, now our total loss is 264.97\n",
      "we're in iteration #37...\n",
      "updating m: 1.56, updating b: 0.79, now our total loss is 252.08\n",
      "we're in iteration #38...\n",
      "updating m: 1.60, updating b: 0.79, now our total loss is 239.87\n",
      "we're in iteration #39...\n",
      "updating m: 1.63, updating b: 0.79, now our total loss is 228.31\n",
      "we're in iteration #40...\n",
      "updating m: 1.67, updating b: 0.79, now our total loss is 217.38\n",
      "we're in iteration #41...\n",
      "updating m: 1.70, updating b: 0.80, now our total loss is 207.02\n",
      "we're in iteration #42...\n",
      "updating m: 1.73, updating b: 0.80, now our total loss is 197.22\n",
      "we're in iteration #43...\n",
      "updating m: 1.76, updating b: 0.80, now our total loss is 187.95\n",
      "we're in iteration #44...\n",
      "updating m: 1.79, updating b: 0.81, now our total loss is 179.17\n",
      "we're in iteration #45...\n",
      "updating m: 1.82, updating b: 0.81, now our total loss is 170.85\n",
      "we're in iteration #46...\n",
      "updating m: 1.85, updating b: 0.81, now our total loss is 162.99\n",
      "we're in iteration #47...\n",
      "updating m: 1.87, updating b: 0.81, now our total loss is 155.54\n",
      "we're in iteration #48...\n",
      "updating m: 1.90, updating b: 0.81, now our total loss is 148.49\n",
      "we're in iteration #49...\n",
      "updating m: 1.93, updating b: 0.82, now our total loss is 141.82\n",
      "we're in iteration #50...\n",
      "updating m: 1.95, updating b: 0.82, now our total loss is 135.50\n",
      "we're in iteration #51...\n",
      "updating m: 1.98, updating b: 0.82, now our total loss is 129.53\n",
      "we're in iteration #52...\n",
      "updating m: 2.00, updating b: 0.82, now our total loss is 123.87\n",
      "we're in iteration #53...\n",
      "updating m: 2.02, updating b: 0.83, now our total loss is 118.51\n",
      "we're in iteration #54...\n",
      "updating m: 2.05, updating b: 0.83, now our total loss is 113.44\n",
      "we're in iteration #55...\n",
      "updating m: 2.07, updating b: 0.83, now our total loss is 108.64\n",
      "we're in iteration #56...\n",
      "updating m: 2.09, updating b: 0.83, now our total loss is 104.10\n",
      "we're in iteration #57...\n",
      "updating m: 2.11, updating b: 0.83, now our total loss is 99.80\n",
      "we're in iteration #58...\n",
      "updating m: 2.13, updating b: 0.84, now our total loss is 95.73\n",
      "we're in iteration #59...\n",
      "updating m: 2.15, updating b: 0.84, now our total loss is 91.87\n",
      "we're in iteration #60...\n",
      "updating m: 2.17, updating b: 0.84, now our total loss is 88.23\n",
      "we're in iteration #61...\n",
      "updating m: 2.19, updating b: 0.84, now our total loss is 84.77\n",
      "we're in iteration #62...\n",
      "updating m: 2.21, updating b: 0.84, now our total loss is 81.51\n",
      "we're in iteration #63...\n",
      "updating m: 2.22, updating b: 0.84, now our total loss is 78.41\n",
      "we're in iteration #64...\n",
      "updating m: 2.24, updating b: 0.85, now our total loss is 75.48\n",
      "we're in iteration #65...\n",
      "updating m: 2.26, updating b: 0.85, now our total loss is 72.71\n",
      "we're in iteration #66...\n",
      "updating m: 2.27, updating b: 0.85, now our total loss is 70.09\n",
      "we're in iteration #67...\n",
      "updating m: 2.29, updating b: 0.85, now our total loss is 67.61\n",
      "we're in iteration #68...\n",
      "updating m: 2.30, updating b: 0.85, now our total loss is 65.26\n",
      "we're in iteration #69...\n",
      "updating m: 2.32, updating b: 0.85, now our total loss is 63.03\n",
      "we're in iteration #70...\n",
      "updating m: 2.33, updating b: 0.85, now our total loss is 60.92\n",
      "we're in iteration #71...\n",
      "updating m: 2.35, updating b: 0.86, now our total loss is 58.93\n",
      "we're in iteration #72...\n",
      "updating m: 2.36, updating b: 0.86, now our total loss is 57.04\n",
      "we're in iteration #73...\n",
      "updating m: 2.38, updating b: 0.86, now our total loss is 55.26\n",
      "we're in iteration #74...\n",
      "updating m: 2.39, updating b: 0.86, now our total loss is 53.57\n",
      "we're in iteration #75...\n",
      "updating m: 2.40, updating b: 0.86, now our total loss is 51.96\n",
      "we're in iteration #76...\n",
      "updating m: 2.41, updating b: 0.86, now our total loss is 50.45\n",
      "we're in iteration #77...\n",
      "updating m: 2.43, updating b: 0.86, now our total loss is 49.01\n",
      "we're in iteration #78...\n",
      "updating m: 2.44, updating b: 0.86, now our total loss is 47.66\n",
      "we're in iteration #79...\n",
      "updating m: 2.45, updating b: 0.87, now our total loss is 46.37\n",
      "we're in iteration #80...\n",
      "updating m: 2.46, updating b: 0.87, now our total loss is 45.16\n",
      "we're in iteration #81...\n",
      "updating m: 2.47, updating b: 0.87, now our total loss is 44.00\n",
      "we're in iteration #82...\n",
      "updating m: 2.48, updating b: 0.87, now our total loss is 42.91\n",
      "we're in iteration #83...\n",
      "updating m: 2.49, updating b: 0.87, now our total loss is 41.88\n",
      "we're in iteration #84...\n",
      "updating m: 2.50, updating b: 0.87, now our total loss is 40.90\n",
      "we're in iteration #85...\n",
      "updating m: 2.51, updating b: 0.87, now our total loss is 39.98\n",
      "we're in iteration #86...\n",
      "updating m: 2.52, updating b: 0.87, now our total loss is 39.10\n",
      "we're in iteration #87...\n",
      "updating m: 2.53, updating b: 0.87, now our total loss is 38.28\n",
      "we're in iteration #88...\n",
      "updating m: 2.54, updating b: 0.88, now our total loss is 37.49\n",
      "we're in iteration #89...\n",
      "updating m: 2.55, updating b: 0.88, now our total loss is 36.75\n",
      "we're in iteration #90...\n",
      "updating m: 2.55, updating b: 0.88, now our total loss is 36.05\n",
      "we're in iteration #91...\n",
      "updating m: 2.56, updating b: 0.88, now our total loss is 35.38\n",
      "we're in iteration #92...\n",
      "updating m: 2.57, updating b: 0.88, now our total loss is 34.75\n",
      "we're in iteration #93...\n",
      "updating m: 2.58, updating b: 0.88, now our total loss is 34.15\n",
      "we're in iteration #94...\n",
      "updating m: 2.59, updating b: 0.88, now our total loss is 33.59\n",
      "we're in iteration #95...\n",
      "updating m: 2.59, updating b: 0.88, now our total loss is 33.06\n",
      "we're in iteration #96...\n",
      "updating m: 2.60, updating b: 0.88, now our total loss is 32.55\n",
      "we're in iteration #97...\n",
      "updating m: 2.61, updating b: 0.88, now our total loss is 32.07\n",
      "we're in iteration #98...\n",
      "updating m: 2.61, updating b: 0.88, now our total loss is 31.62\n",
      "we're in iteration #99...\n",
      "updating m: 2.62, updating b: 0.89, now our total loss is 31.19\n",
      "we're in iteration #100...\n",
      "updating m: 2.63, updating b: 0.89, now our total loss is 30.78\n",
      "we're in iteration #101...\n",
      "updating m: 2.63, updating b: 0.89, now our total loss is 30.40\n",
      "we're in iteration #102...\n",
      "updating m: 2.64, updating b: 0.89, now our total loss is 30.03\n",
      "we're in iteration #103...\n",
      "updating m: 2.65, updating b: 0.89, now our total loss is 29.69\n",
      "we're in iteration #104...\n",
      "updating m: 2.65, updating b: 0.89, now our total loss is 29.36\n",
      "we're in iteration #105...\n",
      "updating m: 2.66, updating b: 0.89, now our total loss is 29.05\n",
      "we're in iteration #106...\n",
      "updating m: 2.66, updating b: 0.89, now our total loss is 28.76\n",
      "we're in iteration #107...\n",
      "updating m: 2.67, updating b: 0.89, now our total loss is 28.48\n",
      "we're in iteration #108...\n",
      "updating m: 2.67, updating b: 0.89, now our total loss is 28.22\n",
      "we're in iteration #109...\n",
      "updating m: 2.68, updating b: 0.89, now our total loss is 27.97\n",
      "we're in iteration #110...\n",
      "updating m: 2.68, updating b: 0.89, now our total loss is 27.74\n",
      "we're in iteration #111...\n",
      "updating m: 2.69, updating b: 0.89, now our total loss is 27.51\n",
      "we're in iteration #112...\n",
      "updating m: 2.69, updating b: 0.89, now our total loss is 27.30\n",
      "we're in iteration #113...\n",
      "updating m: 2.70, updating b: 0.90, now our total loss is 27.10\n",
      "we're in iteration #114...\n",
      "updating m: 2.70, updating b: 0.90, now our total loss is 26.92\n",
      "we're in iteration #115...\n",
      "updating m: 2.70, updating b: 0.90, now our total loss is 26.74\n",
      "we're in iteration #116...\n",
      "updating m: 2.71, updating b: 0.90, now our total loss is 26.57\n",
      "we're in iteration #117...\n",
      "updating m: 2.71, updating b: 0.90, now our total loss is 26.41\n",
      "we're in iteration #118...\n",
      "updating m: 2.72, updating b: 0.90, now our total loss is 26.26\n",
      "we're in iteration #119...\n",
      "updating m: 2.72, updating b: 0.90, now our total loss is 26.11\n",
      "we're in iteration #120...\n",
      "updating m: 2.72, updating b: 0.90, now our total loss is 25.98\n",
      "we're in iteration #121...\n",
      "updating m: 2.73, updating b: 0.90, now our total loss is 25.85\n",
      "we're in iteration #122...\n",
      "updating m: 2.73, updating b: 0.90, now our total loss is 25.72\n",
      "we're in iteration #123...\n",
      "updating m: 2.73, updating b: 0.90, now our total loss is 25.61\n",
      "we're in iteration #124...\n",
      "updating m: 2.74, updating b: 0.90, now our total loss is 25.50\n",
      "we're in iteration #125...\n",
      "updating m: 2.74, updating b: 0.90, now our total loss is 25.40\n",
      "we're in iteration #126...\n",
      "updating m: 2.74, updating b: 0.90, now our total loss is 25.30\n",
      "we're in iteration #127...\n",
      "updating m: 2.75, updating b: 0.90, now our total loss is 25.20\n",
      "we're in iteration #128...\n",
      "updating m: 2.75, updating b: 0.90, now our total loss is 25.12\n",
      "we're in iteration #129...\n",
      "updating m: 2.75, updating b: 0.90, now our total loss is 25.03\n",
      "we're in iteration #130...\n",
      "updating m: 2.76, updating b: 0.91, now our total loss is 24.95\n",
      "we're in iteration #131...\n",
      "updating m: 2.76, updating b: 0.91, now our total loss is 24.88\n",
      "we're in iteration #132...\n",
      "updating m: 2.76, updating b: 0.91, now our total loss is 24.81\n",
      "we're in iteration #133...\n",
      "updating m: 2.76, updating b: 0.91, now our total loss is 24.74\n",
      "we're in iteration #134...\n",
      "updating m: 2.77, updating b: 0.91, now our total loss is 24.68\n",
      "we're in iteration #135...\n",
      "updating m: 2.77, updating b: 0.91, now our total loss is 24.62\n",
      "we're in iteration #136...\n",
      "updating m: 2.77, updating b: 0.91, now our total loss is 24.56\n",
      "we're in iteration #137...\n",
      "updating m: 2.77, updating b: 0.91, now our total loss is 24.51\n",
      "we're in iteration #138...\n",
      "updating m: 2.77, updating b: 0.91, now our total loss is 24.45\n",
      "we're in iteration #139...\n",
      "updating m: 2.78, updating b: 0.91, now our total loss is 24.41\n",
      "we're in iteration #140...\n",
      "updating m: 2.78, updating b: 0.91, now our total loss is 24.36\n",
      "we're in iteration #141...\n",
      "updating m: 2.78, updating b: 0.91, now our total loss is 24.32\n",
      "we're in iteration #142...\n",
      "updating m: 2.78, updating b: 0.91, now our total loss is 24.27\n",
      "we're in iteration #143...\n",
      "updating m: 2.79, updating b: 0.91, now our total loss is 24.23\n",
      "we're in iteration #144...\n",
      "updating m: 2.79, updating b: 0.91, now our total loss is 24.20\n",
      "we're in iteration #145...\n",
      "updating m: 2.79, updating b: 0.91, now our total loss is 24.16\n",
      "we're in iteration #146...\n",
      "updating m: 2.79, updating b: 0.91, now our total loss is 24.13\n",
      "we're in iteration #147...\n",
      "updating m: 2.79, updating b: 0.91, now our total loss is 24.10\n",
      "we're in iteration #148...\n",
      "updating m: 2.79, updating b: 0.91, now our total loss is 24.07\n",
      "we're in iteration #149...\n",
      "updating m: 2.80, updating b: 0.91, now our total loss is 24.04\n",
      "we're in iteration #150...\n",
      "updating m: 2.80, updating b: 0.92, now our total loss is 24.01\n",
      "we're in iteration #151...\n",
      "updating m: 2.80, updating b: 0.92, now our total loss is 23.99\n",
      "we're in iteration #152...\n",
      "updating m: 2.80, updating b: 0.92, now our total loss is 23.96\n",
      "we're in iteration #153...\n",
      "updating m: 2.80, updating b: 0.92, now our total loss is 23.94\n",
      "we're in iteration #154...\n",
      "updating m: 2.80, updating b: 0.92, now our total loss is 23.92\n",
      "we're in iteration #155...\n",
      "updating m: 2.80, updating b: 0.92, now our total loss is 23.90\n",
      "we're in iteration #156...\n",
      "updating m: 2.81, updating b: 0.92, now our total loss is 23.88\n",
      "we're in iteration #157...\n",
      "updating m: 2.81, updating b: 0.92, now our total loss is 23.86\n",
      "we're in iteration #158...\n",
      "updating m: 2.81, updating b: 0.92, now our total loss is 23.84\n",
      "we're in iteration #159...\n",
      "updating m: 2.81, updating b: 0.92, now our total loss is 23.82\n",
      "we're in iteration #160...\n",
      "updating m: 2.81, updating b: 0.92, now our total loss is 23.81\n",
      "we're in iteration #161...\n",
      "updating m: 2.81, updating b: 0.92, now our total loss is 23.79\n",
      "we're in iteration #162...\n",
      "updating m: 2.81, updating b: 0.92, now our total loss is 23.78\n",
      "we're in iteration #163...\n",
      "updating m: 2.81, updating b: 0.92, now our total loss is 23.76\n",
      "we're in iteration #164...\n",
      "updating m: 2.82, updating b: 0.92, now our total loss is 23.75\n",
      "we're in iteration #165...\n",
      "updating m: 2.82, updating b: 0.92, now our total loss is 23.74\n",
      "we're in iteration #166...\n",
      "updating m: 2.82, updating b: 0.92, now our total loss is 23.72\n",
      "we're in iteration #167...\n",
      "updating m: 2.82, updating b: 0.92, now our total loss is 23.71\n",
      "we're in iteration #168...\n",
      "updating m: 2.82, updating b: 0.92, now our total loss is 23.70\n",
      "we're in iteration #169...\n",
      "updating m: 2.82, updating b: 0.92, now our total loss is 23.69\n",
      "we're in iteration #170...\n",
      "updating m: 2.82, updating b: 0.92, now our total loss is 23.68\n",
      "we're in iteration #171...\n",
      "updating m: 2.82, updating b: 0.92, now our total loss is 23.67\n",
      "we're in iteration #172...\n",
      "updating m: 2.82, updating b: 0.92, now our total loss is 23.66\n",
      "we're in iteration #173...\n",
      "updating m: 2.82, updating b: 0.92, now our total loss is 23.66\n",
      "we're in iteration #174...\n",
      "updating m: 2.82, updating b: 0.92, now our total loss is 23.65\n",
      "we're in iteration #175...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.64\n",
      "we're in iteration #176...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.63\n",
      "we're in iteration #177...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.63\n",
      "we're in iteration #178...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.62\n",
      "we're in iteration #179...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.61\n",
      "we're in iteration #180...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.61\n",
      "we're in iteration #181...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.60\n",
      "we're in iteration #182...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.60\n",
      "we're in iteration #183...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.59\n",
      "we're in iteration #184...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.59\n",
      "we're in iteration #185...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.58\n",
      "we're in iteration #186...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.58\n",
      "we're in iteration #187...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.57\n",
      "we're in iteration #188...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.57\n",
      "we're in iteration #189...\n",
      "updating m: 2.83, updating b: 0.93, now our total loss is 23.56\n",
      "we're in iteration #190...\n",
      "updating m: 2.84, updating b: 0.93, now our total loss is 23.56\n",
      "we're in iteration #191...\n",
      "updating m: 2.84, updating b: 0.93, now our total loss is 23.56\n",
      "we're in iteration #192...\n",
      "updating m: 2.84, updating b: 0.93, now our total loss is 23.55\n",
      "we're in iteration #193...\n",
      "updating m: 2.84, updating b: 0.93, now our total loss is 23.55\n",
      "we're in iteration #194...\n",
      "updating m: 2.84, updating b: 0.93, now our total loss is 23.55\n",
      "we're in iteration #195...\n",
      "updating m: 2.84, updating b: 0.93, now our total loss is 23.54\n",
      "we're in iteration #196...\n",
      "updating m: 2.84, updating b: 0.93, now our total loss is 23.54\n",
      "we're in iteration #197...\n",
      "updating m: 2.84, updating b: 0.93, now our total loss is 23.54\n",
      "we're in iteration #198...\n",
      "updating m: 2.84, updating b: 0.93, now our total loss is 23.53\n",
      "we're in iteration #199...\n",
      "updating m: 2.84, updating b: 0.93, now our total loss is 23.53\n",
      "we're in iteration #200...\n",
      "updating m: 2.84, updating b: 0.93, now our total loss is 23.53\n",
      "Final m: 2.84, Final b: 0.93\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "# Gradient descent loop\n",
    "m_values = [m]\n",
    "b_values = [b]\n",
    "loss_values = [mse_loss(m, b)]\n",
    "print(f\"starting with m: {m:.2f}, b: {b:.2f}, now our total loss is {mse_loss(m,b):.2f}\")\n",
    "for i in range(iterations):\n",
    "    print(f\"we're in iteration #{i+1}...\")\n",
    "    y_pred = linear_model(X,m,b)\n",
    "    error = y_pred - Y\n",
    "\n",
    "    # Compute gradients\n",
    "    dm = (2/len(X)) * np.sum(error * X)\n",
    "    db = (2/len(X)) * np.sum(error)\n",
    "\n",
    "    # Update parameters\n",
    "    m -= learning_rate * dm\n",
    "    b -= learning_rate * db\n",
    "    print(f\"updating m: {m:.2f}, updating b: {b:.2f}, now our total loss is {mse_loss(m,b):.2f}\")\n",
    "\n",
    "\n",
    "    m_values.append(m)\n",
    "    b_values.append(b)\n",
    "    loss_values.append(mse_loss(m, b))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Final m: {m:.2f}, Final b: {b:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "830361ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: regression line\n",
    "ax1.scatter(X, Y, color='blue')\n",
    "line, = ax1.plot(X, m_values[0]*X + b_values[0], 'r-', linewidth=2)\n",
    "ax1.set_title(\"Gradient Descent Fit\")\n",
    "\n",
    "# Right: loss over iterations\n",
    "ax2.set_xlim(0, len(loss_values))\n",
    "ax2.set_ylim(0, max(loss_values)*1.1)\n",
    "loss_line, = ax2.plot([], [], 'g-', linewidth=2)\n",
    "ax2.set_title(\"Loss over Iterations\")\n",
    "ax2.set_xlabel(\"Iteration\")\n",
    "ax2.set_ylabel(\"MSE Loss\")\n",
    "\n",
    "# Animation function\n",
    "def animate(i):\n",
    "    line.set_ydata(m_values[i]*X + b_values[i])\n",
    "    loss_line.set_data(range(i+1), loss_values[:i+1])\n",
    "    return line, loss_line\n",
    "\n",
    "ani = FuncAnimation(fig, animate, frames=len(m_values), interval=100, blit=True)\n",
    "\n",
    "ani.save(\"linear_regression_graphs.gif\", writer='pillow', fps=10)\n",
    "\n",
    "plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a207d9",
   "metadata": {},
   "source": [
    "![Gradient Descent Animation](linear_regression_graphs.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7667077f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started with loss function of 1766.5031899691112 and ended up minimizing the loss with GD to 23.527202283466462\n"
     ]
    }
   ],
   "source": [
    "print(f'started with loss function of {loss_values[0]} and ended up minimizing the loss with GD to {loss_values[-1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
